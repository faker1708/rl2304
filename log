

现在的想法是
1   保存神经网络，而不保存整个dqn对象
2   想办法用pid 算法把村杆子移到中间去。
3   以后的训练可以在已经保存模型上继续训练，相当于读档。




发现不需要几百万成绩的模型,5万的来测试,基本也可以非常非常稳定了.
我认为可以在奖励函数 中添加位移的积分.明天 来试试.大概率 没什么d用.



1万不行
2万可以.



现在这个 已经可以把位移压在一个很小的范围内了。0.1以内
方法是
1   添加pid算法，目前只加了积分。没有加微分
2   先开着epsilon = 0.9 训练出一个跑分不错，而且x很小的模型
3   读取这个模型，把epsilon关了，也就是设置为1。然后再训练一遍
4   读取后面这个模型，运行即可。



发现，貌似用eps = 1 来训练，小车的位移并不能向0收敛。一直处于一个固定的值周围振荡。

哦，它是多周目收敛，单周目是不收敛的。太神奇了。

eps = 1是可以收敛的。



忘了说了。我们还修改了积分函数。
模仿强化学习那个gamma的公式。
本来积分就是累加，但我不想要太久远的记忆，于是考虑用数组更新。
然后又想到可以用gamma来让最新的数据占比更大。这样就是一个不一样的积分函数了。


实际 使用起来，至少是能用的。也基本满足我的需求。

所以说，多学点各式各样的知识，确实有用啊。在很多不起眼的地方就能派上大用场。



新需求
考虑添加x绝对值 的最大值 。因为x过大，游戏就结束 ，所以x的最大值 也需要压一压。

顺便直接在训练时让x的均值方差都收敛就得了。

能控制输出的均值方差的话，那真的很厉害。







