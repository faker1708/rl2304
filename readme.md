

cart_pole实验

功能

让小车稳定在初始位置附近

原理：用pid算法改进了奖励函数


复现操作：
1   将train 的dqn_init 改成1
2   将train 的 保存模型阈值改成1

跑几轮生成一个模型 命名为a

3   将train 的dqn_init 改成0，读取a模型
4   将train 的 保存模型阈值改成0.24 或者更小

跑几轮生成一个模型 命名为b

5   load b 这个模型



不足之处：位置是稳定了，但游戏失败率增加了。之前的模型基本没见过失败的。现在的模型往往会失败。
但好处是一旦稳定运行，就可以宣布游戏不会结束了，杆子不会倒下，小车也不会偏太远。





一个重要的思想是，我们的思想要加在神经网络之前 ，要把它训练到神经网络里面，而不是训练好之后 再去外部干预。比如这次添加的pid算法。
我后来就是直接写在奖励函数 中，收获 了不错的效果。


pid算法，我也有所改进。所以，一步一步，都不容易，都是探索着前进。并不是一开始就有明确的方向。




思考 。
其实就是猴子敲打字机，你随机训练 ，一定能找到满意的。但是不知道什么时候才能找到。

所以，我们算法要做的，就是让机器积极地向着我们的需求去收敛，而不是随机试验。那样太抽奖了。不可靠。







